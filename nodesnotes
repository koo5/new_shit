

"""
#Natural deduction aka logic programming
def build_in_nat_deduct(r):
	




#Full tau/MLTT logic, better lang than the last one, more or less like agda
#but maybe prettier or uglier
#-------------------
#named expressions
#type declarations and object definitions; type-checking "distinct" from type-inference
#(parameterized) dependent record types
#(parameterized) inductive types
#opening (parameterized) records/modules
#universe hierarchy (like HMC's)
def build_in_agdaTau(r):
"""
"""
	
	#parenthesized expressions
	#vars
	#applications
	#abstractions
	#Level and numbers
	#Set / universes
	#Succ
	#Prev
	#Max
	#pi-types
	#sigma-types
	#DRTs
	#open
	#inductive data types
	#named expressions
		#type signature
		#definition
	
"""

"""
#CNL:
#instead of making the agda-like version, just
#make the CNL version directly; the reasoning would be the
#same, just the parsing would be different.
"""
#def build_in_CNL(r):
"""
this probably calls for something besides context-free grammar.

"""


"""
#Monads and imperative?

#modal logic
	#probably
	#possibly
	#necessarily
	#maybe

#HoTT: Types are infinity groupoids! Equivalence is equivalent to identity


	
	"""
        class IntType(Syntaxed):
                help = ["integer type"]
        build_in(SyntaxedNodecl(IntType,
                [TextTag("int")],
                {}), None, lc2)
        class BoolType(Syntaxed):
                help = ["bool type"]
        build_in(SyntaxedNodecl(BoolType,
                [TextTag("bool")],
                {}), None, lc2)

	"""
"""

Expr 		:=	Expr1 | "(" Expr ")"
Expr1		:=	Var | Abs | App
Var		:= 	Char Name # Identifier
Abs		:=	"|" Var "." Expr
App		:=	Expr " " Expr
Name		:=	<endword> | Char Name
Char		:=	"a" | "b" | "c" | ... | 
			"A" | "B" | ... |
			"1" | "2" | ... | 
			"_" ...

//possibly
NamedExpr	:=	Name ":=" Expr

so i guess the question wrt syntax is how do you tell a named expression
from a var or do you, if not, 
i'm thinking you can treat everything that's syntactically the same as
a var as "potentially a named expression"
but, it gets kinda syntactically messy when you can have things like

myFunc := ...

myOtherFunc := \\myFunc.myFunc
		^ for example, myFunc in this expression would be
		taken as a var, even though myFunc is defined before it
		
myThirdFunc := \\x.(myFunc x)
		  ^ the name doesn't correspond to a bound var, so
		  treat it as the func
		  
it's perhaps slightly contrived, but it is unambiguous at least,
and rather simple

i don't think  we should make implementing anything contrived be
a priority tho. to me, the main critical piece is to be able to
make a full spectrum of logic systems, from the utlc up through
fully dependently typed lambda calc, i.e. MLTT or even HoTT

secondary for me is sugaring to interface to these logics
(at least until we've implemented the different logics, then
this one will become primary for me)

so..we need a table right? like the thunks table
yea, at the very least we need a table of expression definitions
so that the evaluator can call them up when needed
if the expressions in this table can have their definitions updated
when they get evaluated, and newly-found expressions can be added
to the table, then we basically have haskell's table of thunks
and we basically have lazy evaluation
newly-found? so, once the expression starts evaluating then it
can generate 'new' expressions over the course of its evaluation
umm
you mean it can evaluate to a new expression to be put in its place or
something else? both, the something else would be evaluating to an
expression that contains multiple instances of the same sub-expression,
where these sub-expressions "haven't been seen before", so they'd
essentially get a new entry in the table

i'm not aware to what extents haskell does the second part
ahh so this is an optimization only? i mean the repeated expression "elimination"
yep that's basically all lazy evaluation is hmm

according to wiki "is an evaluation strategy which delays the evaluation
of an expression until its value is needed (non-strict evaluation), 
and which also avoids repeated evaluations (sharing)."

so, it's basically the evaluation strategy we have hooked up already,
combined with this repeated-expressions optimization

ok yea i see kinda...
im ready to crash
sounds good, anything else would be too much to get into tonight anyway
yeah half past two, i should
i guess tomorrow before i wake up give some thought to what you wanna
implement next, i'm gonna keep focusing on the plans for the simply
typed lambda in the meantime tho
sounds good
getting an idea of proper typing does sound more important
for us i think so
especially cause as we proceed into the dependent typing this
intersects back with tau, so, killing multiple birds and all that
alright
catch you tomorrow later


does haskell have different naming schemes?
well, haskell has the whole typing thing so they get a lot more
info about what the expression is supposed to be

and then to actually evaluate the expression they use the
pattern matching thing, and i think that doing these things
they basically make it completely unambiguous

so..what do you suggest?

one potential unambiguous naming scheme would be to treat
names that have a corresponding expression as expressions unless
that name is a bound var

yea we could do that, couple things to figure out with it,
first thing is getting a complete syntax for it

would this be used like
it would be like
"def Name:
	Expr"
<some expression using Name>
more definitions
more expressions


or just like


definition 1
definition 2
oneandonlyexpression

probably more like

definition 1
definition 2
def Main:	<--but yea this is essentially our
		"one and only expr"
	...

if you add more expressions to evaluate that's just
basically running multiple programs in sequence
(but perhaps reusing definitions between them)
well, at least that's a "typical interpretation" of the
situation

could also interpret it as running a sequence of commands, all
within the same program

but... *shrug*

i think the more critical point is that by introducing named
expressions we might actually be making it something slightly
different than 'proper' lambda calculus

notice that a lambda expression cannot be recursive
the language doesn't even have the capacity to express a recursion
recursive behavior can be recovered through use of clever arguments
like
(\\a.(a a)) (\\a.(a a)) --eval-to--> (\\a.(a a)) (\\a.(a a))

but this is a different mechanism than

def myFunc := \\a.(myFunc a)

ok, so i understand it as a whole new macro language
that's probably the most appropriate interpretation

*scratches head* wrt anything in particular?
the motivation i guess, behind lambda calc?
behind the naming thing
well, makes it more like a real prog-lang,
it's something i just thought to add before we really
got into it and i saw that these kinds of recursions aren't
actually part of the lambda calc itself

so, can we perhaps look at haskell and define the semantics of this 
new language? the new language being this utlc + the naming thing?
kinda just the naming thing separately ah
yea, could look at just about any lang for this
haskell is like utlc + naming thing + simple typing + sugar

this naming thing actually has to be mada a part of the evaluator right?
yea, if we tried to macro expand with essentially recursively defined
macros, then... yea
haskell doesn't really have a problem with that because they're already
storing a bunch of thunks for their lazy evaluation
hmm not sure what that means but i guess ill see later

i'm not 100% how the details work, but haskell basically stores an
index to references to partially evaluated expressions, called thunks, 
so if the same sub-expression is used multiple times in the 
super-expression, then if you evaluate the first instance of the
sub-expression, you've evaluated all instances, because the other
instances are just referencing this thunk that's now been evaluated

so, they're already handling all this stuff at the evaluator level
anyway just to implement lazy evaluation
ah





//not all applications are function applications, cf. "y y"
but the var could evaluate to a function
unless we've reached normal form, i.e. nothing left to evaluate
so is this a syntax for normal form or nonnormal form?
could be either one. 

|x.(x x) y  --reduce--> y y, and nothing left to evaluate so this is in normal form
    ^ also occurs in the original expression here

|y.(|x.(x x) y) |w.w --reduce -->
|y.(y y) |w.w  --reduce -->
    ^ here it occurs, but it's not in normal form

lambda calculus is quite hard to follow
i agree. it's mainly only of theoretical interest until you get
some relatively sophisticated syntax for it
i would probably start without the redex/nonredex distinction so we can meaningufully express even nonnormal programs, hrm, i don't think we need the redex/nonredex distinction for the syntax
|w.w |w.w --reduce-->
|w.w



1) Parse query according to BNF above

2) If it parses, then normalize the expression


//Need to choose reduction strategy:
//There are several options:
/* * Full beta reduction: non-normalizing
	Any reducible expression can be reduced at any time.
   	This is essentially the lack of any particular reduction
	strategy, or equivalently the union of all reduction strategies.


   * Applicative order: non-normalizing
	Rightmost, innermost reducible expression is always reduced first.
	This means a function's arguments are always reduced before the
	function itself.

   * Normal order:
	The leftmost, outermost reducible expression is always reduced first.

   * Call by name:
	Same as normal order except no reductions are performed inside abstractions.
	For example: |x.((|x.x) x) would be considered to be in normal form, even
	though it contains the reducible expression ((|x.x) x). Since this redex
	occurs inside an abstraction, it is not to be reduced.

   * Call by value:
	?

   * Call by need:
	aka "lazy evaluation"
	Function applications that would duplicate terms will name the argument
	instead, which is then reduced only "when it is needed", and which will
	represent all duplicated versions of itself. i.e. if you reduce one of
	the duplicates, then they are all reduced, as they will just be names
	referring to an expression that is now reduced.

	This is the one to use, but the tradeoff is it's more complex to
	implement.	

*/		

//aka evaluate
//Normal order:
Normalize (Expr e) :=
	if (e is a Var x)
		return e
	if (e is an Abs x e1)
		return (Abs x normalize(e1))
	if (e is an App e1 e2)
		if(e1 is an Abs x e3)
			//one-step evaluation:
			return subst(e3,x,e2)
		ah so we should pick one here	
			//full evaluation:
			return normalize(subst(e3,x,e2))
		if(e1 is an App e3 e4)
			
		else return e



//applicative order:
Normalize (Expr e) :=
	if (e is a Var x)
		return e
	if (e is an Abs x e1)
		return e
	if (e is an App e1 e2)
		if(e1 is an Abs x e3)
			//first check if the rightmost expression is reducible
			if(isReducible(e2))
				//one-step evaluation
				return (App e1 normalize(e2))

				//full evaluation
				return normalize((App e1 normalize(e2)))

			//if not, then check if the innermost expression is
			//reducible
			if(isReducible(e3))
				//one-step evaluation
				return (App (Abs x normalize(e3)) e2)

				//full evaluation
				return normalize((App (Abs x normalize(e3)) e2))

			//neither the rightmost nor innermost expressions are
			//reducible
			return subst(e3,x,e2)
		else return e


//call by name
Normalize (Expr e) :=
	if (e is a Var x)
		return e
	//unlike normal order we don't normalize inside abstractions
	if (e is an Abs x e1)
		return e
	if (e is an App e1 e2)
		if (e1 is an Abs x e3)
			//one-step evaluation:
                        return subst(e3,x,e2)

                        //full evaluation:
                        return normalize(subst(e3,x,e2))
                else return e


//call by value
evaluate arguments to functions first before substituting them into
the function

this actually not one single evaluation strategy but is the family of
evaluation strategies where the arguments are normalized before substituting
them into the function. 

here's one example that does this but follows the normal order evaluation
strategy otherwise


Normalize (Expr e) :=
	if (e is a Var x)
		return e
	if (e is an Abs x e1)
		return e
	if (e is an App e1 e2)
		if(isReducible(e2))
			//one step evaluation
			return (App e1 normalize(e2))

			//full evaluation
			return normalize(App e1 normalize(e2))
		if (e1 is an Abs x e3)
			//one step evaluation
			return subst(e3,x,e2)
		
			//full evaluation
			return normalize(subst(e3,x,e2))
		else return e



//call by need (aka lazy evaluation)
need some mechanism for storing and retrieving the thunks
expressions look different internally: composed of references
to thunks




//call by reference

//call by sharing

//call by copy-restore

//call by future

//optimistic evaluation

//partial evaluation




An expression is reducible if contains anything that can be reduced.
The only things that can be reduced are of the form (App (Abs x e1) e2)

//The expression contains a "top-level" reducible expression
//meaning the reducible expression does not occur inside the body
//of an abstraction.
isReducible(Expr e) :=
	if(e is a Var x) return false
	if(e is a Abs x e1) return false
	if(e is a App e1 e2)
		if(e1 is a Abs x e3) return true
		if(e1 is a Var x) return false
		if(isReducible(e1)) return true
		if(isReducible(e2)) return true
		else return false
	
//The expression contains a reducible expression anywhere,
//possibly in the body of an abstraction.
isReducible2(Expr e) :=
	if(e is a Var x) return false
	if(e is a Abs x e1) return isReducible2(e1)
	if(e is a App e1 e2)
		if(e1 is a Abs x e3) return true
		if(e1 is a Var x) return false
		if(isReducible2(e1)) return true
		if(isReducible2(e2)) return true
		else return false


//capture-avoiding substitution:
subst(Expr e, Var x, Expr r) :=
	replace all occurrences of x in e with r, potentially
	changing variable names to avoid variable capture.

	capture-avoidance:
	don't let free variables become bound
	
/*
	//This is the var we're substituting, so make
	//the substitution
	(Var x)[x := r]		= r
	
	//This is a different var than the one we're substituting,
	//don't substitute anything
	(Var y)[x := r]		= y

	//Substitute into each component of an application:
	(App t s)[x := r]	= (t[x:=r])(s[x:=r])
				= App subst(t,x,r) subst(s,x,r)

	//if the substitution-variable is the same as the bound-variable
	//of an abstraction, then don't substitute, even in the expression t.
	//why?
	//answer 1: just because
	//answer 2: ??
	(Abs x t)[x := r]	= (Abs x t).

	
	//naive version:
	//if the substitution-variable is *not* the same as the bound-variable
	//of an abstraction, then substitute [x := r] in t
	//there is nothing to avoid variable capture here.
	(Abs y t)[x := r]	= (Abs y subst(t,x,r))


	//capture-avoiding version:
	//if the substitution-variable is *not* the same as the bound-variable
	//of an abstraction, then if y is not in the FreeVars of r, then
	//apply the substitution in the body of the abstraction.
	//if y *is* in the FreeVars of r, then we'll rename y to a
	//fresh variable name, z, and substitute z for y across the
	//entire abstraction.
	//then we will proceed with our usual substitution of r for x,
	//as defined in the naive version.
	(Abs y t)[x := r]	= 
		if(y not in FreeVars(r)) -> (Abs y subst(t,x,r))
		else
			z = genFreshVarName()
			t' = subst(t,y,z)
			t'' = subst(t',x,r)
			return (Abs z t'')


*/


FreeVars(Expr e, List<Var> * ret) :=
	if (e is a Var x) 
		ret = set_union(ret,{x})
		return
	if (e is a Abs x e1)
		free_e := FreeVars(e1)
		remove x from free_e
		ret = set_union(ret,free_e)
		return
	if (e is a App e1 e2)
		free_e1 := FreeVars(e1)
		free_e2 := FreeVars(e2)
		ret = set_union(ret,free_e1,free_e2)
		return



//eta-conversion

f == |x.(f x)

//not really necessary. not sure where this would even come up in normal
//programming with lambdas. for theorem proving otoh, there's reason to use it

















# cool i guess that's everything from the bnf then
# yeah next i have to finish the parser
# cool so how does what we have here fit into that
#well, the gui is set up so that when you go into lc1-test, it
#collects the grammar from the nodes in its scope

the above module now looks like this in the gui:
[lc1]:<module[]from[]:
<<->[ 
    <syntactic category:[exp]>
    <var:
        a variable
        example: <{}>>
    <parexp:
        a parenthesized expression
        example: <({})>>
    <<*parexp>works as<*exp>>
    <abs:
        abstraction, a lambda
        example: <|
        
        
        {}.{}>>
    <app:
        function application, a call
        example: <{} {}>>
    ]>end.>

the "example:"'s dont work very well yet
it doesnt even display the syntaxes, too










this is the grammar of the particular node type
		[TextTag("("), ChildTag("exp"), TextTag(")")],
it has to go over this list
ah it goes over the list to generate a grammar for marpa
yeah to call marpa .symbol and .rule functions








-       #i need to think a bit if we need to do anything different for our base types
-       #but, not sure, maybe we should just start working on the logic and it might
-       #become more clear
-       #in tau HMC doesn't even want to have these types explicitly,
-       #apparently everything can be made with pi's and sigmas
-       #the article also doesn't give any mention to these so... need to think it over a bit
-       #i would guess they wont be a problem but no idea
-       #i'm gonna assume that if the article doesn't use them then they're unnecessary
-       #cause he does use base types in the simply typed lambda, but then they seem to
-       #mysteriously disappear in the lambda cube section, and his example expressions
-       #don't make any use of anything except the syntax we've already got laid out above
-       #mm, then lets get it working without i guess





	so....abs has a BuiltinNodecl
	and this nodecl would lemonishly be Pitype
	and thats probably all i can say about all this because i just have no idea
	i have ParametricNodecls, for example; so these correspond to "type operators" in the
	type theory stuff: "\\t:*.(List t)" and "t:*->*"
	well, we gotta keep doing it the nonlemonish way at least until i see such type operators in 
	action and really understand it
	well, we can start doing stuff like that already, using that church-encoding stuff
	to represent our data-types
	well if you think its a good next step, sure
	hrm, nothing needs to be changed in the system to allow it, it's just special lambda expressions
	that we can already write
	something that i lack a feel for is, is this lambda calculus with church encoding 
	something that we would then really use in a real system?
	probably internally, but almost surely not explicitly in the syntax of a real lang
	i would imagine not interally either because speed?
	speed is an issue, imo this should be handled by getting to the level that the system
	can kind of understand the translation between the two
	so it seems like, we will be working on a different level, only making sure that there
	exists some theoretical correspondence..i dunno
	pretty much yea
	so im not sure i want to go figuring out how this lc-way maps to ... not sure what really-Lemonish representation something,
	i should have said, figuring out how we would go about representing/projecting these bits of lc
	to a higher/lemon-like level , if we then perhaps wouldnt really use the lc representation
	at the moment the lc representation is what we know how to *reason* over, which is it's main utility
	it can be considered training wheels until we know how to reason over the same stuff in a better
	represenation, what's particularly useful about it is that everything has homogeneous representation:
	just lambda expressions. we can change whatever we want, but we don't want to break that
	homogeneousness (or the nice logical/computational properties of MLTT)
	alright
	
	... i'm more concerned of how it maps to machine code (at this stage at least)
	ok, so..yeah
	i have to get a better handle on church-encoding in MLTT myself
	so..lets just keep going with this separately from the lemon mess
tgh	alri
	
	with these i can express types like list of numbers
	and i can declare the type itself  / express that there can be such a type
	btw this is separate from the Refs stuff, which are just something like identifiers or something
	


"""





master = Dotdict()
master._dict = odict()
def build_in_master(r):
	r["master"] = new_module()
	r["master"].ch.statements.items = [
		Parser("""
		superframe is a kind of thing
		master is a superframe
		slave is a superframe
		#"to" would be equivalent with "command"
		to show slave:
			run python code: server_frames.slave.show()
		to up:
			-||-
		to do <command> <number> times:
			for i from 0 to number:
				command
		to go <direction> <number> times:
			for i from 0 to number:
				
		direction is a kind of thing
		up is a direction
		down is a direction
		
		...
		command menu:
			
		
		
		
		
		""")]

